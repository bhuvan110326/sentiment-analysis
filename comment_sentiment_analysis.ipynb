{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3e3057e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter the YouTube video URL: https://youtu.be/zuegQmMdy8M\n",
      "Extracted Video ID: zuegQmMdy8M\n",
      "Positive Comments: 696\n",
      "Negative Comments: 128\n",
      "Neutral Comments: 416\n",
      "\n",
      "Top Sentences in Negative Comments:\n",
      "!: 8 times - Translation: !\n",
      "?: 3 times - Translation: ?\n",
      "Sorry.: 2 times - Translation: Sorry.\n",
      "this rick that got fired?: 1 times - Translation: this rick that got fired?\n",
      "OKay I will explain the pointers to you and you will rate it.: 1 times - Translation: OKay I will explain the pointers to you and you will rate it.\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.probability import FreqDist\n",
    "from nltk import pos_tag\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from googletrans import Translator\n",
    "import re\n",
    "from googleapiclient.discovery import build\n",
    "\n",
    "\n",
    "def extract_video_id(video_url):\n",
    "    # Match various YouTube URL formats\n",
    "    match = re.search(r'(?:youtube\\.com\\/(?:[^\\/\\n\\s]+\\/\\S+\\/|(?:v|e(?:mbed)?)\\/|\\S*?[?&]v=)|youtu\\.be\\/)([a-zA-Z0-9_-]{11})', video_url)\n",
    "    return match.group(1) if match else None\n",
    "\n",
    "# Replace 'YOUR_VIDEO_URL' with the actual YouTube video URL\n",
    "video_url = input(\"Enter the YouTube video URL: \")\n",
    "videoid = extract_video_id(video_url)\n",
    "\n",
    "print(\"Extracted Video ID:\", videoid)\n",
    "\n",
    "\n",
    "from googleapiclient.discovery import build\n",
    "\n",
    "def get_video_comments(api_key, video_id):\n",
    "    youtube = build('youtube', 'v3', developerKey=api_key)\n",
    "\n",
    "    # Get comments from the video\n",
    "    request = youtube.commentThreads().list(\n",
    "        part=\"snippet\",\n",
    "        videoId=video_id,  # Add the videoId parameter\n",
    "        textFormat=\"plainText\",\n",
    "        maxResults=100\n",
    "    )\n",
    "    comments = []\n",
    "    while request:\n",
    "        response = request.execute()\n",
    "        for item in response.get('items', []):\n",
    "            comment = item['snippet']['topLevelComment']['snippet']['textDisplay']\n",
    "            comments.append(comment)\n",
    "\n",
    "        request = youtube.commentThreads().list_next(request, response)\n",
    "\n",
    "    return comments\n",
    "\n",
    "\n",
    "def analyze_sentiment(comment):\n",
    "    analyzer = SentimentIntensityAnalyzer()\n",
    "    sentiment_score = analyzer.polarity_scores(comment)['compound']\n",
    "\n",
    "    if sentiment_score >= 0.05:\n",
    "        return 'Positive'\n",
    "    elif sentiment_score <= -0.05:\n",
    "        return 'Negative'\n",
    "    else:\n",
    "        return 'Neutral'\n",
    "\n",
    "# Function to preprocess and analyze negative comments\n",
    "\n",
    "\n",
    "\n",
    "# Function to suggest positive replacements for negative words\n",
    "def get_positive_suggestion(negative_word):\n",
    "    # Replace negative words with positive ones (this is a simplified example)\n",
    "    positive_word_dict = {\n",
    "        'bad': 'good',\n",
    "        'hate': 'love',\n",
    "        'negative_word_3': 'positive_word_3',  # Add more replacements as needed\n",
    "    }\n",
    "    return positive_word_dict.get(negative_word, 'positive_word')\n",
    "    \n",
    "\n",
    "# Replace 'YOUR_API_KEY' and 'YOUR_VIDEO_ID' with your actual API key and video ID\n",
    "api_key = 'AIzaSyAgJqt2a__JoDyBnCcIJnZJazIZXcaNd54'\n",
    "video_id = videoid\n",
    "comments = get_video_comments(api_key, video_id)\n",
    "\n",
    "positive_count = 0\n",
    "negative_count = 0\n",
    "neutral_count = 0\n",
    "\n",
    "for comment in comments:\n",
    "    sentiment = analyze_sentiment(comment)\n",
    "    if sentiment == 'Positive':\n",
    "        positive_count += 1\n",
    "    elif sentiment == 'Negative':\n",
    "        negative_count += 1\n",
    "    else:\n",
    "        neutral_count += 1\n",
    "\n",
    "# Display the counts\n",
    "print(\"Positive Comments:\", positive_count)\n",
    "print(\"Negative Comments:\", negative_count)\n",
    "print(\"Neutral Comments:\", neutral_count)\n",
    "\n",
    "# Analyze and display negative topics and suggestions\n",
    "def analyze_negative_comments(comments):\n",
    "    negative_comments = [comment for comment in comments if analyze_sentiment(comment) == 'Negative']\n",
    "\n",
    "    # Tokenize into sentences\n",
    "    all_sentences = []\n",
    "    for comment in negative_comments:\n",
    "        sentences = sent_tokenize(comment)  # <-- Tokenize into sentences\n",
    "        all_sentences.extend(sentences)\n",
    "\n",
    "    # Calculate sentence frequencies\n",
    "    sentence_freq = {}\n",
    "    for sentence in all_sentences:\n",
    "        if sentence in sentence_freq:\n",
    "            sentence_freq[sentence] += 1\n",
    "        else:\n",
    "            sentence_freq[sentence] = 1\n",
    "\n",
    "    # Display the top 5 most frequent sentences\n",
    "    top_sentences = sorted(sentence_freq.items(), key=lambda x: x[1], reverse=True)[:5]\n",
    "    \n",
    "    # Suggest translations for the top sentences\n",
    "    translator = Translator()\n",
    "    suggestions = {}\n",
    "    for sentence, frequency in top_sentences:\n",
    "        try:\n",
    "            # Translate the sentence to English\n",
    "            translated_sentence = translator.translate(sentence, src='auto', dest='en').text\n",
    "            suggestions[sentence] = {'frequency': frequency, 'translation': translated_sentence}\n",
    "        except:\n",
    "            pass  # Ignore translation errors\n",
    "\n",
    "    return suggestions\n",
    "\n",
    "suggestions = analyze_negative_comments(comments)\n",
    "\n",
    "if suggestions:\n",
    "    print(\"\\nTop Sentences in Negative Comments:\")\n",
    "    for sentence, data in suggestions.items():\n",
    "        print(f\"{sentence}: {data['frequency']} times - Translation: {data['translation']}\")\n",
    "else:\n",
    "    print(\"\\nNo negative comments found.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3505e4f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3e818e69",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\bhuva\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('vader_lexicon')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "904c0214",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting googletrans==4.0.0-rc1\n",
      "  Downloading googletrans-4.0.0rc1.tar.gz (20 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting httpx==0.13.3 (from googletrans==4.0.0-rc1)\n",
      "  Downloading httpx-0.13.3-py3-none-any.whl (55 kB)\n",
      "     ---------------------------------------- 0.0/55.1 kB ? eta -:--:--\n",
      "     ---------------------------------------- 55.1/55.1 kB 2.8 MB/s eta 0:00:00\n",
      "Requirement already satisfied: certifi in c:\\users\\bhuva\\downloads\\anaconda.app\\lib\\site-packages (from httpx==0.13.3->googletrans==4.0.0-rc1) (2023.7.22)\n",
      "Collecting hstspreload (from httpx==0.13.3->googletrans==4.0.0-rc1)\n",
      "  Obtaining dependency information for hstspreload from https://files.pythonhosted.org/packages/b5/9f/83329ebd2808e04f2564051e4c4a880a1e2e67bd6410899f728096d0e22f/hstspreload-2024.2.1-py3-none-any.whl.metadata\n",
      "  Downloading hstspreload-2024.2.1-py3-none-any.whl.metadata (2.1 kB)\n",
      "Requirement already satisfied: sniffio in c:\\users\\bhuva\\downloads\\anaconda.app\\lib\\site-packages (from httpx==0.13.3->googletrans==4.0.0-rc1) (1.2.0)\n",
      "Collecting chardet==3.* (from httpx==0.13.3->googletrans==4.0.0-rc1)\n",
      "  Downloading chardet-3.0.4-py2.py3-none-any.whl (133 kB)\n",
      "     ---------------------------------------- 0.0/133.4 kB ? eta -:--:--\n",
      "     -------------------------------------- 133.4/133.4 kB 8.2 MB/s eta 0:00:00\n",
      "Collecting idna==2.* (from httpx==0.13.3->googletrans==4.0.0-rc1)\n",
      "  Downloading idna-2.10-py2.py3-none-any.whl (58 kB)\n",
      "     ---------------------------------------- 0.0/58.8 kB ? eta -:--:--\n",
      "     ---------------------------------------- 58.8/58.8 kB ? eta 0:00:00\n",
      "Collecting rfc3986<2,>=1.3 (from httpx==0.13.3->googletrans==4.0.0-rc1)\n",
      "  Downloading rfc3986-1.5.0-py2.py3-none-any.whl (31 kB)\n",
      "Collecting httpcore==0.9.* (from httpx==0.13.3->googletrans==4.0.0-rc1)\n",
      "  Downloading httpcore-0.9.1-py3-none-any.whl (42 kB)\n",
      "     ---------------------------------------- 0.0/42.6 kB ? eta -:--:--\n",
      "     ---------------------------------------- 42.6/42.6 kB ? eta 0:00:00\n",
      "Collecting h11<0.10,>=0.8 (from httpcore==0.9.*->httpx==0.13.3->googletrans==4.0.0-rc1)\n",
      "  Downloading h11-0.9.0-py2.py3-none-any.whl (53 kB)\n",
      "     ---------------------------------------- 0.0/53.6 kB ? eta -:--:--\n",
      "     ---------------------------------------- 53.6/53.6 kB ? eta 0:00:00\n",
      "Collecting h2==3.* (from httpcore==0.9.*->httpx==0.13.3->googletrans==4.0.0-rc1)\n",
      "  Downloading h2-3.2.0-py2.py3-none-any.whl (65 kB)\n",
      "     ---------------------------------------- 0.0/65.0 kB ? eta -:--:--\n",
      "     ---------------------------------------- 65.0/65.0 kB ? eta 0:00:00\n",
      "Collecting hyperframe<6,>=5.2.0 (from h2==3.*->httpcore==0.9.*->httpx==0.13.3->googletrans==4.0.0-rc1)\n",
      "  Downloading hyperframe-5.2.0-py2.py3-none-any.whl (12 kB)\n",
      "Collecting hpack<4,>=3.0 (from h2==3.*->httpcore==0.9.*->httpx==0.13.3->googletrans==4.0.0-rc1)\n",
      "  Downloading hpack-3.0.0-py2.py3-none-any.whl (38 kB)\n",
      "Downloading hstspreload-2024.2.1-py3-none-any.whl (1.1 MB)\n",
      "   ---------------------------------------- 0.0/1.1 MB ? eta -:--:--\n",
      "   ---------------- ----------------------- 0.5/1.1 MB 14.2 MB/s eta 0:00:01\n",
      "   ------------------------ --------------- 0.7/1.1 MB 8.7 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 0.9/1.1 MB 6.9 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 1.0/1.1 MB 6.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------  1.1/1.1 MB 6.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.1/1.1 MB 5.0 MB/s eta 0:00:00\n",
      "Building wheels for collected packages: googletrans\n",
      "  Building wheel for googletrans (setup.py): started\n",
      "  Building wheel for googletrans (setup.py): finished with status 'done'\n",
      "  Created wheel for googletrans: filename=googletrans-4.0.0rc1-py3-none-any.whl size=17461 sha256=6498b70a16c1b212760aef484fe5f499ab7df796427e510080ce6049a5ff543e\n",
      "  Stored in directory: c:\\users\\bhuva\\appdata\\local\\pip\\cache\\wheels\\39\\17\\6f\\66a045ea3d168826074691b4b787b8f324d3f646d755443fda\n",
      "Successfully built googletrans\n",
      "Installing collected packages: rfc3986, hyperframe, hpack, h11, chardet, idna, hstspreload, h2, httpcore, httpx, googletrans\n",
      "  Attempting uninstall: chardet\n",
      "    Found existing installation: chardet 4.0.0\n",
      "    Uninstalling chardet-4.0.0:\n",
      "      Successfully uninstalled chardet-4.0.0\n",
      "  Attempting uninstall: idna\n",
      "    Found existing installation: idna 3.4\n",
      "    Uninstalling idna-3.4:\n",
      "      Successfully uninstalled idna-3.4\n",
      "Successfully installed chardet-3.0.4 googletrans-4.0.0rc1 h11-0.9.0 h2-3.2.0 hpack-3.0.0 hstspreload-2024.2.1 httpcore-0.9.1 httpx-0.13.3 hyperframe-5.2.0 idna-2.10 rfc3986-1.5.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "transformers 2.1.1 requires sentencepiece, which is not installed.\n"
     ]
    }
   ],
   "source": [
    "pip install googletrans==4.0.0-rc1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e1144256",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\bhuva\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "265cf145",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
